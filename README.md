# Indian-Sign-Language-AI
A model that can recognize the hand signs made by deaf and mute people to translate and teach to all people.

# Tech Stack
1. MediaPipe to recognize the hands.
2. LSTM algorithm to make sure the sequence of signs is remembered.
3. OpenCV for capturing video of signs.

# Approach
We used mediapipe as a backbone model as it is pre-trained on how to recognize hands. It is a model trained by Google so we choose this over CNN as it is light and efficient for this use case.
On top of MediaPipe we are using LSTM model which is being trained on how to detect the signs. It works by understanding the pattern of vectors generated by the mediapipe model. We normalized the vectors after removing the mapping point of wrist. WHY?

> Experience
> We faced a issue where model was remembering the distance of the sign being made from. Meaning, if we trained the model with far distance then the model won't recognize signs with close distance. This issue was solved by doing two things: removing the wrist point which was acting as a centre point of the hand. And secondly, flattening the vector so that model only focuses on the signs being made and not the location where the signs were being made.

Doing so we acheived this:
1. Accuracy went from 93% to 97%.
2. Model was recognizing the signs irrespective of the place they were being made.

# Data
1. Main question is, "Where did you find the data train the model?"
   Answer: We didn't find any suitable dataset that could work for our purpose. So we thought of creating the dataset on our own. We recorded videos of 1 second for signs that were quick to make, converted them into ".npy" format for model training. 

> .npy files are easy to store and manage. Yes, we can't figure out which image it is but surely model does.

2. How many signs have we trained?
   Answer: Around 15-20 signs and we are still working on increasing the accuracy per sign and number of signs.
4. How much data you have used?
   Answer: Approximately, 1500 images per sign at this stage. 
