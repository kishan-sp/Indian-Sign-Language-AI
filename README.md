# Version History
- ISL_Model_v1 - 20 signs

---

# Indian Sign Language AI

A machine learning model designed to recognize, translate, and teach Indian Sign Language (ISL), bridging the communication gap between the Deaf community and the hearing world.

## Tech Stack

* **MediaPipe:** For robust, pre-trained hand tracking and landmark recognition.
* **LSTM (Long Short-Term Memory):** To process and remember the sequence of hand movements over time.
* **OpenCV:** For real-time video capture and frame processing.

## Our Approach

We chose **MediaPipe** as our backbone model because it comes pre-trained for hand recognition. Since it is developed by Google and highly optimized, it provided a lighter, more efficient alternative to training a heavy Convolutional Neural Network (CNN) from scratch for this specific use case.

On top of MediaPipe, we built an **LSTM model** trained to detect and interpret the actual signs. It works by understanding the pattern of coordinate vectors generated by MediaPipe. Crucially, we normalized these vectors after removing the wrist mapping point. *Why did we do this?*

> **The Challenge & The Solution**
> We ran into a frustrating issue where the model was memorizing the *distance* of the hand from the camera rather than the sign itself. If we trained the model on signs made far away, it completely failed to recognize those same signs when made up close! 
>
> We solved this by doing two things:
> 1. We removed the wrist point, which was acting as an absolute center point for the hand.
> 2. We flattened and normalized the coordinate vectors. This forced the model to focus purely on the relative shape and movement of the hand, completely ignoring its spatial location on the screen.

**The Results:**
* Accuracy jumped from **93% to 97%**.
* The model can now recognize signs flawlessly, regardless of where they are positioned in the camera frame.

## Data Collection

One of the biggest hurdles in this project was data. 

**1. Where did we find the training data?**
We couldn't find a suitable existing dataset for our specific needs, so we decided to build our own! We recorded 1-second videos for quick signs and converted the frame data into `.npy` (NumPy) format for model training.
> *Note: `.npy` files are incredibly efficient to store and manage. While humans can't easily visualize them like standard images, our model processes them perfectly.*

**2. How many signs have we trained?**
Currently, the model recognizes around **15 to 20 signs**. We are actively working on increasing both the accuracy of these signs and our overall vocabulary.

**3. How much data was used?**
At this stage, we have captured approximately **1,500 frames or images per sign**.

**4. How did we record all this data?**
We wrote a custom Python script to automate the process. The script prompts the user for the sign's name, captures exactly 1 second of video, and saves the landmark data into a designated folder in NumPy format. Because we record at 30 FPS, each 1-second capture yields 30 frames per sign, capturing both the static pose and the actual sequence of movement.

> **Why Motion and Clarity Matter:** > Sign language relies heavily on both movement and static hand shapes. The LSTM is a sequential model built specifically to remember the order of data, making it perfect for capturing the *motion* of the sign. Meanwhile, the static frame captures provide the high-quality positional data the model needs to understand the exact hand shape.

## Future Scope & Contributing

We believe the LSTM model will be incredibly useful for translating longer, continuous sentences due to its natural capability to remember sequential data. However, we also know there is always room to learn and improve!

We are very open to contributions, advice, and collaborations. If you have experience in this field, notice a knowledge gap we can fill, or just want to help build something meaningfulâ€”we would love to have you on board!
