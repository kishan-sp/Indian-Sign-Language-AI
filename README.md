# Indian-Sign-Language-AI
A model that can recognize the hand signs made by deaf and mute people to translate and teach to all people.

# Tech Stack
1. MediaPipe to recognize the hands.
2. LSTM algorithm to make sure the sequence of signs is remembered.
3. OpenCV for capturing video of signs.

# Approach
We used mediapipe as a backbone model as it is pre-trained on how to recognize hands. It is a model trained by Google so we choose this over CNN as it is light and efficient for this use case.
On top of MediaPipe we are using LSTM model which is being trained on how to detect the signs. It works by understanding the pattern of vectors generated by the mediapipe model. We normalized the vectors after removing the mapping point of wrist. WHY?

> Experience
> We faced a issue where model was remembering the distance of the sign being made from. Meaning, if we trained the model with far distance then the model won't recognize signs with close distance. This issue was solved by doing two things: removing the wrist point which was acting as a centre point of the hand. And secondly, flattening the vector so that model only focuses on the signs being made and not the location where the signs were being made.

Doing so we acheived this:
1. Accuracy went from 93% to 97%.
2. Model was recognizing the signs irrespective of the place they were being made.

# Data
1. Main question is, "Where did you find the data train the model?"
- Answer: We didn't find any suitable dataset that could work for our purpose. So we thought of creating the dataset on our own. We recorded videos of 1 second for signs that were quick to make, converted them into ".npy" format for model training. 

> .npy files are easy to store and manage. Yes, we can't figure out which image it is but surely model does.

2. How many signs have we trained?
- Answer: Around 15-20 signs and we are still working on increasing the accuracy per sign and number of signs.
3. How much data you have used?
- Answer: Approximately, 1500 images per sign at this stage. 
4. How did we record these many data?
- Answer: We ran a python script that asks for the name of the sign, captures video for 1 second, stores it in the folder of the sign's name in the numpy format. Each second contains 30 fps leading to 30 images for 1 sign with actual sequence of movement and the static image of sign.

> We need 2 things here: Motion and Clarity. LSTM is a sequential model which is specifically made to remember the order to data, here we use static images to give model the high quality image of how the hand sign will look like. Secondly, video sequence will provide the motion of signs wehn being made.

# Scope
We think that LSTM model will be useful for long sentences considering its capability of remembering the order of data. But, we might be wrong or lack knowledge in this terms so we are open to any kind of contributions and collaborations.
